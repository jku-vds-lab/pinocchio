<main class="container-fluid">
    <h1>Mutliple Testing</h1>
    <section>
        <p>Statistics is defined as the Science of studying and developing methods for intelligent analysis of data.
            This methods are then used for many different other sciences to gain knowledge. In Economics one talks about Econometrics which is a combination of Statistics and Research-Designs. In Natural Sciences one describes experiments and their outcomes through statistics. Medicine needs to test new drugs for their effectiveness. Basically one could argue: without statistics there is no science and no journalism.
        </p>

        <p>The more one thinks about the importance of statistics the more important it is educate people about the traps and problems when using statistics in an uninformed way!</p>

        <p>Just consider this picture below: Does it make sense?</p>
        <div class="img_cont">
            <img src="assets/chart.png">
        </div>

        Every reasonable person must appreciate this is none-sense!
    </section>
    <section>
        <h2>Inferential - Statistics</h2>

        <p>Two types of statistics exist, we have descriptive statistics and inferential statistics</p>
        <aside class="row">
            <div class="col-sm-6">
                <strong>Descriptive Statistics</strong>
                <p>Describing Data, especially ground populations.</p>
                <p>classical summary statistics like: mean, median, variance, standard-deviation, mode, distibution functions (cdf, pdf), graphs</p>
            </div>
            <div class="col-sm-6">
                <strong>Inferential Statistics</strong>
                <p>Draw a sample (representative) and conclude how the population behaves!</p>

                <div class="img_cont">
                    <img src="https://statsandr.com/blog/what-is-the-difference-between-population-and-sample_files/population-sample.png">
                </div>
                <p>clever sampling strategies, to ensure representative samples. Weighting of samples, non-response, research designs, ...</p>
                <p>use the same sumary statistics but with caution. Bias adjusted versions (e.g. population vs. sample variance), working more with probabilities, statistical tests</p>
            </div>
        </aside>
    </section>
    <section>
        <h2>Correlation and Causation</h2>
        <p>The above image already implies what correlation is, a quick reminder what correlation is!</p>
        <p>Correlation is a statistical dependence i.e. you can infer something about datapoints y dependent on datapoint x</p>
        <p>Classical example: higher BMI (body mass index) correlates with higher incidence of certain diseases! <br> But however is this realy causal? Do just higher weight imply this diseases?</p>

        <p>Causation however means that something is indeed the reason for the effect(correlation) is a causal relationship and not by chance. To distinguish between things occuring by chance and founded evidence one has to do statistical tests and clever-research designs.</p>

        <div class="img_cont">
            <img src="assets/bald_men.png">
        </div>
        <p>Intresting!!! But is this true? Can you explain it by natural science?</p>

        <p>Find two graphs below, the first graph shows you what the news-article suggests. The second graph shows an alternative interpretation. To actually find out what is now the causal relationship is very hard, one would need high degree of domain knowledge to try to explain it by theory and also as mentioned multiple times, clever research designs. </p>
        <div class="img_cont" id="sp2">
            <img src="assets/causation_correlation.png">
        </div>

        <p>by the way for your intrest these graphs are called DAGs - directed acyclic graphs and are often used for showing causal relationships</p>

    </section>
    <section>
        <h2>Sometimes pets and small children are right even then when all evidence says something different</h2>

        <div class="img_cont" id="sp3">
            <img id="sp_1" src="https://media.cheggcdn.com/media/ae5/ae5fb628-dbe6-4af5-a82f-8ab754d8befc/php9t3Ybz">
        </div>
    </section>
    <section>
        <h2>A statistical application</h2>

        <p>However lets leave the question of correlation and causation as its root alone for now. Its a very important and difficult question in different sciences and subject to clever research designs which are most-likely even domain specific!</p>
        <p>Lets look on statistical tests and multiple testing. Inferential statistics tries to infer knowledge based on observations done on data samples.
            The key idea is the assumption that if we draw a representative sample it will have overall the same statistic properties as the ground population. The key problem of course is randomness. If one would have some technique which answers how probable it is whether this result is by random chance or really true.
        </p>
        <p>A very good example to understand  inferential statistics and statistical tests is a medical drug effect study.
            You draw two samples which have the same properties as the ground population.

            Now the drug is prescribed to the first group while some placebo is served to the second group.

            Now one formulates two hypothesis:

            <br>
            H_0: here is no difference between the groups - the drug has no effect
            <br>
            H_1: there is some difference between the groups - the drug causes more health, less pain whatsoever
            <br>

            In the end we want to reject the null-hypothesis i.e we want to be sure that some drug has the effect.

            <br>
            Basically now we can compare for example the means (and other descriptive statistics like median, variance, mode, ...)

            e.g. the treated group has an average pain-level of 10 while the untreated group has an average pain-level of 7.

            A reasonable assumption now is to assume that the drug had one effect.

            But please analyse the picture below:
        </p>

        <div class="img_cont">
            <img src="assets/Untitled2.gif">
        </div>
        <p>Okay now lets look a bit closer, it seems that the drug has an effect the mean diverges after the treatment. Ok Null-Hypothesis rejected perfect drug will be produced and selled to all people around the world right? Hmm not quite right? For now dismiss that if there is some effect it is quite small but when thinking about it more closely it could be that the change in pain-level is just by chance. Just look at the picture below to have more time analysing it. You see everywhere there is some probabilities (for selection, for distributing to the different groups, how advanced the ilness already is, general health condistion, ...) involved. So maybe its just by chance as already said. And now comes our statistical testing. </p>
        <div class="img_cont">
            <img src="assets/medical.png">
        </div>
    </section>

    <section>
        <h2>Statistical Significance</h2>

        <p>Basically we now that probabilities follow some distribution, we can assume for reasons based on probability theory that our values for an estimator (like mean, ...) is distributed via a normal distribution</p>
        <p>i.e our expected value which we got above is the best guess for the ground-truth-expected value. This value is distributed normally with some certain variance.</p>

        <p>now we can build around our estimated value a confidence interval which just the interval between the true value has to lie with a specific probability.</p>

        <p>Its more or less a common-ground in science to assume that everything between 95% is significant</p>

        <div class="img_cont">
            <img src="assets/geogebra-export.png">
        </div>
        <p>So basically when the 95% intervals of both distributions (i.e. the one of the control group and the one of the treatment group) do not overlap we can be sure that with 95% probability when repeating the experiment over and over again we still will have a significant difference. </p>
        <!--<input type="range" value="{{this.v}}" class="form-control" min="0" max="6">-->
        <div>
            <canvas height="50%" baseChart
                    [data]="scatterChartData2"
                    [options]="scatterChartOptions2"
                    [type]="scatterChartType2">
            </canvas>
        </div>

    </section>
    <section>
        <h2>Take a quick look on one testing-method "students t test"</h2>
    </section>
    <section>
        <h2>A little experiment</h2>
        <p>Now conduct some experiment! Basically we copy the graph from above, dark blue is our control group while cyan is our treatment. Now we generate with the parameters of our cyan curve more curves but the new mean values are also distributed normaly with N(4, 0.01) so now we get different curves. </p>
        <p>And normally you should see in the green coloured curves that they are still significantly different but suddenly there are also in a more red-brownish colors they are suddenly not any more significantly different. </p>
        <p><small><u>Disclaimer: the below graph is generated by random i.e the result depends on the random numbers generated and can be different each time.</u></small></p>
        <app-some-chart></app-some-chart>
    </section>

    <section>
        <h2>Overview of multiple testing</h2>

        <p>Multiple testing is for example important in natural sciences i.e. when considering DNA-Research (<a href="https://en.wikipedia.org/wiki/Microarray">microarray-analysis</a>)</p>

        <p>So again what is now the problem: we find significant relationships when doing t-tests but in reality they are not there. The problem increases with the number of experiments/estimations.</p>

        <p>Again the idea is the following: we have a black box and each time doing experiments we have some certain probability to get a false-positive. This is what I wanted to show you with the above normal distribution experiment. We have some probability to get other than the values and indeed we get them.</p>
        <p>To illustrate look at the following, initally we have a probabilty to get a right result which is quite high but than wen repeating all over again the probability for a wrong result increases significantly. </p>

        <div class="img_cont">
            <img src="assets/experi2.gif">
        </div>

        <p>Wanna try on some real data? On your own dataset? I have prepared some little prototype where you can play around and find significant data. <br> You can be sure that when having enough attributes you can find some significant relationship (on linear regression/OLS) which is complete non-sense </p>
        <a class="btn btn-primary" routerLink="/upload">Go to data upload</a><br>
        <a class="btn btn-primary" routerLink="/manual">Go directly to playaround</a><br>
        <a class="btn btn-primary" routerLink="/automatic">Go directly to automatic scan</a><br>

        <p>So we can conclude now the following -> automatic scanning can be dangerous.</p>

    </section>

    <section>
        <h2>Strategy for Multiple Testing I</h2>
        <p>Actually the most safest strategy is simple. Whenever doing data-driven research one should first have some theory.</p>
        <p>So always think first, what could be, what you are searching for, what does domain-specific theory say? Then make confirmatory data-analytics and only do analytics which is "meaningful". Of course this is rather limited. Often we do not have a theory and past research or the data is too abstract to use common-sensen. A very important field is exploratory data-analytics which just means searching for data and then try to explain the results. This exploratory data analytics is not possible with this rather simple approach.</p>

        <div class="img_cont">
            <img src="https://datos.gob.es/sites/default/files/u322/grafico.jpg">
        </div>


    </section>

    <section>
        <h2>Strategy for Multiple Testing II</h2>
        <p>For doing exploratory data analysis where we do huge-scale multiple testing an option would be to adjust the p-value.</p>
        <p>Multiple testing correction adjusts the p-value such that the overall result (i.e. probability of false positive) is smaller than the specified significane level (normally 95%). </p>

        <p>But of course these is not the holy-grale because depending on which exact countermeasure you take you still increase also the False Negatives rate potentially. i.e. since you adjust your p-value you than say suddenly - no this is not significant even if it would be in reality. So kind-of the inverse problem</p>
        <p>A researcher has to decide which problem is more serious- potentially missing a big hit or potentially finding something which is indeed bullshit.</p>
        <div class="img_cont">
            <img src="assets/graph_correction.png">
            <p><small>Taken from: https://physiology.med.cornell.edu/people/banfelder/qbio/resources_2008/1.5_GenespringMTC.pdf</small></p>

        </div>

        <h3>Bonferroni correction</h3>
        <p>This is a very simple and strict approach. The idea is to multiply the value we compare with with the number of already done tests. Here the number of misses will be quite high since we drease very fast</p>

        <p>p_new = p * n</p>

        <p>Consider the following row of comparisions</p>
        <div class="img_cont">
            <img src="assets/row.png">
            <p><small>The significance level = 0.05 and the p value resulting would be 0,01. This value will be multiplied over again and suddenly we see that even if the outcome would not change we go over the border rather fast.</small></p>
        </div>

        <h3>Bonferroni Step-down (Holm) correction </h3>
        <p>An adopted version of aboves correction. The experiments outcomes (p-values) are ordered from smallest to largest. The first one is calculated as p_new = p*n < 0.05. The next as p_new = p*(n-1) <0.05. And so on. </p>

        <h3>Westfall and Young Permutation </h3>
        <p>In contrast to single-step methods, where each experiment is corrected independently we have methods which permutates all at the same time</p>
        <p>P-values are calculated for each experiment based on the original data set, then we split the data-set into parts, new p-values are calculated for each data-set, the minima are compared to the original ones, the smaller p_values are used at each comparision. </p>
        <h3>Benjamini and Hochberg False Discovery Rate</h3>
        <p>The experiments outcomes (p-values) are ordered from smallest to largest. The largest p-value is accepted as it is. The second one is multiplied by the fraction of #experiments / </p>

    </section>



    <p>...</p>
    <h3>t-Test in Linear Regression</h3>
    <p>For statistical tests we need always two hypothesis! A null hypothesis we want to reject.
        In linear regression it is of course a bit more difficult.

        Generally one needs to evaluate if there is a statistically significant relationship between the x and y.
        <br>
        So we have
        <br>
        <br>
        <b>H0: β1 = 0 i.e (the slope is equal to zero)</b><br>
        <b>HA: β1 != 0 (the slope is not equal to zero)</b>
    </p>

    <p>
        Caclulate the test statistic <br>
        t = b / SE_b <br>

        where we have<br>
        b: coefficient<br>
        SE_b: standard error of the coefficient estimate



    </p>

    <p>
        If the p-value that corresponds to t is less than some threshold (e.g. α = .05) then we reject the null hypothesis and conclude that there is a statistically significant relationship between the predictor variable and the response variable.

        The following example shows how to perform a t-test for a linear regression model in practice.
    </p>
    <p>https://www.reed.edu/economics/course_pages.archive/red_spots/testing_hypotheses.htm</p>
    <p>https://vitalflux.com/linear-regression-t-test-formula-example/</p>
    <p>https://www.statology.org/t-test-linear-regression/</p>




</main>